#!/usr/bin/env python3
"""
Real Performance Test - Download models and test actual performance
Tests both ASR (Whisper) and LLM (Qwen) with real models
"""
import time
import numpy as np
import os
from typing import Dict, List

def generate_test_audio(duration_seconds: float, sample_rate: int = 16000) -> np.ndarray:
    """Generate synthetic audio for testing (sine wave)"""
    samples = int(duration_seconds * sample_rate)
    t = np.linspace(0, duration_seconds, samples, dtype=np.float32)
    # Mix of frequencies to simulate speech
    audio = (
        0.3 * np.sin(2 * np.pi * 200 * t) +
        0.2 * np.sin(2 * np.pi * 400 * t) +
        0.15 * np.sin(2 * np.pi * 800 * t) +
        0.1 * np.random.randn(samples).astype(np.float32)
    )
    return audio.astype(np.float32)

def test_llm_download_and_speed():
    """Download Qwen model and test generation speed"""
    print("\n" + "="*70)
    print("TEST 1: LLM (Qwen 0.5B) - Descarga y Velocidad")
    print("="*70)
    
    try:
        from llama_cpp import Llama
        
        print("\nğŸ“¥ Descargando modelo Qwen 0.5B...")
        print("   Repo: Qwen/Qwen2.5-0.5B-Instruct-GGUF")
        print("   Archivo: qwen2.5-0.5b-instruct-q4_k_m.gguf")
        
        download_start = time.time()
        llm = Llama.from_pretrained(
            repo_id="Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            filename="qwen2.5-0.5b-instruct-q4_k_m.gguf",
            n_ctx=2048,
            n_threads=os.cpu_count() or 4,
            verbose=False
        )
        download_time = time.time() - download_start
        print(f"âœ… Modelo descargado e inicializado en {download_time:.2f}s")
        
        # Test prompts
        test_prompts = [
            "What did she say?",
            "How should I respond?",
            "Is this a question?",
        ]
        
        print("\nğŸ§ª Probando velocidad de generaciÃ³n...")
        latencies = []
        
        for i, prompt in enumerate(test_prompts):
            user_text = f"She said: '{prompt}'"
            
            start = time.time()
            output = llm(
                f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n",
                max_tokens=50,
                temperature=0.7,
                stop=["<|im_end|>"],
                echo=False
            )
            latency = time.time() - start
            latencies.append(latency)
            
            response = output['choices'][0]['text'].strip()
            
            if i == 0:
                print(f"\n  Ejemplo de respuesta:")
                print(f"  Input: {user_text}")
                print(f"  Output: {response[:100]}...")
                print(f"  Latencia: {latency*1000:.0f}ms")
        
        avg_latency = np.mean(latencies)
        min_latency = np.min(latencies)
        max_latency = np.max(latencies)
        
        print(f"\nğŸ“Š Resultados LLM:")
        print(f"  â€¢ Prompts probados: {len(test_prompts)}")
        print(f"  â€¢ Latencia promedio: {avg_latency*1000:.0f}ms")
        print(f"  â€¢ Latencia mÃ­nima: {min_latency*1000:.0f}ms")
        print(f"  â€¢ Latencia mÃ¡xima: {max_latency*1000:.0f}ms")
        
        if avg_latency < 0.5:
            print(f"  â€¢ Estado: ğŸš€ EXCELENTE (<500ms)")
        elif avg_latency < 1.0:
            print(f"  â€¢ Estado: âœ… BUENO (<1s)")
        else:
            print(f"  â€¢ Estado: âš ï¸ ACEPTABLE (>1s)")
        
        return {
            "success": True,
            "download_time": download_time,
            "avg_latency": avg_latency,
            "min_latency": min_latency,
            "max_latency": max_latency
        }
        
    except Exception as e:
        print(f"âŒ Error en test LLM: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def test_asr_download_and_speed():
    """Download Whisper model and test transcription speed"""
    print("\n" + "="*70)
    print("TEST 2: ASR (Whisper tiny.en) - Descarga y Velocidad")
    print("="*70)
    
    try:
        from faster_whisper import WhisperModel
        
        print("\nğŸ“¥ Descargando modelo Whisper tiny.en...")
        
        download_start = time.time()
        model = WhisperModel(
            "tiny.en",
            device="cpu",
            compute_type="int8",
            cpu_threads=os.cpu_count() or 4,
            num_workers=1
        )
        download_time = time.time() - download_start
        print(f"âœ… Modelo descargado e inicializado en {download_time:.2f}s")
        
        # Generate test audio
        print("\nğŸµ Generando audio de prueba...")
        test_audios = [
            generate_test_audio(0.8),   # 800ms
            generate_test_audio(1.5),   # 1.5s
            generate_test_audio(2.0),   # 2s
            generate_test_audio(1.0),   # 1s
        ]
        total_audio_duration = 0.8 + 1.5 + 2.0 + 1.0
        print(f"âœ… Generado {len(test_audios)} chunks de audio ({total_audio_duration}s total)")
        
        # Warm-up
        print("\nğŸ”¥ Calentando modelo...")
        _ = model.transcribe(test_audios[0], language="en", vad_filter=False, beam_size=1)
        
        # Test transcription speed
        print("\nğŸ§ª Probando velocidad de transcripciÃ³n...")
        latencies = []
        
        for i, audio in enumerate(test_audios):
            audio_duration = len(audio) / 16000.0
            
            start = time.time()
            segments, _ = model.transcribe(
                audio,
                language="en",
                vad_filter=False,
                beam_size=1,
                best_of=1,
                temperature=0.0,
                condition_on_previous_text=False,
                without_timestamps=True,
                log_progress=False
            )
            text = " ".join(seg.text.strip() for seg in segments).strip()
            latency = time.time() - start
            latencies.append(latency)
            
            if i == 0:
                print(f"\n  Ejemplo de transcripciÃ³n:")
                print(f"  Audio duraciÃ³n: {audio_duration*1000:.0f}ms")
                print(f"  Latencia: {latency*1000:.0f}ms")
                print(f"  RTF: {latency/audio_duration:.2f}x")
        
        avg_latency = np.mean(latencies)
        min_latency = np.min(latencies)
        max_latency = np.max(latencies)
        total_processing_time = sum(latencies)
        realtime_factor = total_processing_time / total_audio_duration
        
        print(f"\nğŸ“Š Resultados ASR:")
        print(f"  â€¢ Chunks procesados: {len(test_audios)}")
        print(f"  â€¢ Audio total: {total_audio_duration:.2f}s")
        print(f"  â€¢ Tiempo procesamiento: {total_processing_time:.2f}s")
        print(f"  â€¢ Latencia promedio: {avg_latency*1000:.0f}ms")
        print(f"  â€¢ Latencia mÃ­nima: {min_latency*1000:.0f}ms")
        print(f"  â€¢ Latencia mÃ¡xima: {max_latency*1000:.0f}ms")
        print(f"  â€¢ RTF (Real-Time Factor): {realtime_factor:.2f}x")
        
        if realtime_factor < 0.3:
            print(f"  â€¢ Estado: ğŸš€ EXCELENTE (RTF < 0.3)")
        elif realtime_factor < 0.5:
            print(f"  â€¢ Estado: âœ… BUENO (RTF < 0.5)")
        elif realtime_factor < 1.0:
            print(f"  â€¢ Estado: âš ï¸ ACEPTABLE (RTF < 1.0)")
        else:
            print(f"  â€¢ Estado: âŒ LENTO (RTF >= 1.0)")
        
        return {
            "success": True,
            "download_time": download_time,
            "avg_latency": avg_latency,
            "min_latency": min_latency,
            "max_latency": max_latency,
            "realtime_factor": realtime_factor
        }
        
    except Exception as e:
        print(f"âŒ Error en test ASR: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def test_full_pipeline():
    """Test the full pipeline (ASR + LLM) with real models"""
    print("\n" + "="*70)
    print("TEST 3: Pipeline Completo (ASR â†’ LLM)")
    print("="*70)
    
    try:
        from faster_whisper import WhisperModel
        from llama_cpp import Llama
        
        # Load models (should be cached now)
        print("\nğŸ“¦ Cargando modelos (desde cache)...")
        
        load_start = time.time()
        asr_model = WhisperModel("tiny.en", device="cpu", compute_type="int8", 
                                 cpu_threads=os.cpu_count() or 4, num_workers=1)
        asr_load_time = time.time() - load_start
        print(f"âœ… ASR cargado en {asr_load_time:.2f}s")
        
        load_start = time.time()
        llm = Llama.from_pretrained(
            repo_id="Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            filename="qwen2.5-0.5b-instruct-q4_k_m.gguf",
            n_ctx=2048,
            n_threads=os.cpu_count() or 4,
            verbose=False
        )
        llm_load_time = time.time() - load_start
        print(f"âœ… LLM cargado en {llm_load_time:.2f}s")
        
        # Simulate real conversation
        print("\nğŸ­ Simulando conversaciÃ³n real...")
        
        # Generate audio for "What do you think about this proposal?"
        audio = generate_test_audio(2.5)  # 2.5s of audio
        audio_duration = len(audio) / 16000.0
        
        print(f"\n1ï¸âƒ£ Audio entrada: {audio_duration:.2f}s")
        
        # Step 1: ASR
        asr_start = time.time()
        segments, _ = asr_model.transcribe(
            audio, language="en", vad_filter=False, beam_size=1,
            best_of=1, temperature=0.0, condition_on_previous_text=False,
            without_timestamps=True, log_progress=False
        )
        transcribed_text = " ".join(seg.text.strip() for seg in segments).strip()
        asr_time = time.time() - asr_start
        
        print(f"2ï¸âƒ£ ASR: {asr_time*1000:.0f}ms")
        print(f"   Transcrito: (audio sintÃ©tico)")
        
        # Step 2: LLM
        llm_start = time.time()
        output = llm(
            f"<|im_start|>system\nYou are a helpful assistant in a conversation.<|im_end|>\n<|im_start|>user\nSomeone said something. Suggest a brief response.<|im_end|>\n<|im_start|>assistant\n",
            max_tokens=40,
            temperature=0.7,
            stop=["<|im_end|>"],
            echo=False
        )
        response = output['choices'][0]['text'].strip()
        llm_time = time.time() - llm_start
        
        print(f"3ï¸âƒ£ LLM: {llm_time*1000:.0f}ms")
        print(f"   Respuesta: {response[:80]}...")
        
        total_time = asr_time + llm_time
        
        print(f"\nğŸ“Š Resultados Pipeline Completo:")
        print(f"  â€¢ DuraciÃ³n audio: {audio_duration:.2f}s")
        print(f"  â€¢ Tiempo ASR: {asr_time*1000:.0f}ms")
        print(f"  â€¢ Tiempo LLM: {llm_time*1000:.0f}ms")
        print(f"  â€¢ Tiempo TOTAL: {total_time*1000:.0f}ms")
        print(f"  â€¢ RTF Pipeline: {total_time/audio_duration:.2f}x")
        
        if total_time < 0.5:
            print(f"  â€¢ Estado: ğŸš€ EXCELENTE (<500ms)")
        elif total_time < 1.0:
            print(f"  â€¢ Estado: âœ… BUENO (<1s)")
        else:
            print(f"  â€¢ Estado: âš ï¸ ACEPTABLE (>1s)")
        
        return {
            "success": True,
            "asr_time": asr_time,
            "llm_time": llm_time,
            "total_time": total_time,
            "audio_duration": audio_duration,
            "rtf": total_time / audio_duration
        }
        
    except Exception as e:
        print(f"âŒ Error en test pipeline: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         TEST REAL DE RENDIMIENTO - MODELOS DESCARGADOS             â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    print("\nğŸ¯ Este test descarga modelos reales y mide velocidad real")
    print("   â€¢ LLM: Qwen 0.5B Q4_K_M (~300MB)")
    print("   â€¢ ASR: Whisper tiny.en (~75MB)")
    print()
    
    results = {}
    
    # Test 1: LLM
    results['llm'] = test_llm_download_and_speed()
    
    # Test 2: ASR
    results['asr'] = test_asr_download_and_speed()
    
    # Test 3: Full Pipeline
    if results['llm']['success'] and results['asr']['success']:
        results['pipeline'] = test_full_pipeline()
    
    # Final Summary
    print("\n" + "="*70)
    print("RESUMEN FINAL - TESTS CON MODELOS REALES")
    print("="*70)
    
    if results['llm']['success']:
        print(f"\nâœ… LLM (Qwen 0.5B):")
        print(f"   â€¢ Descarga: {results['llm']['download_time']:.2f}s")
        print(f"   â€¢ Latencia promedio: {results['llm']['avg_latency']*1000:.0f}ms")
        
    if results['asr']['success']:
        print(f"\nâœ… ASR (Whisper tiny.en):")
        print(f"   â€¢ Descarga: {results['asr']['download_time']:.2f}s")
        print(f"   â€¢ Latencia promedio: {results['asr']['avg_latency']*1000:.0f}ms")
        print(f"   â€¢ RTF: {results['asr']['realtime_factor']:.2f}x")
    
    if 'pipeline' in results and results['pipeline']['success']:
        print(f"\nâœ… Pipeline Completo:")
        print(f"   â€¢ Tiempo total: {results['pipeline']['total_time']*1000:.0f}ms")
        print(f"   â€¢ RTF: {results['pipeline']['rtf']:.2f}x")
    
    print("\n" + "="*70)
    
    success_count = sum(1 for r in results.values() if r.get('success', False))
    total_count = len(results)
    
    if success_count == total_count:
        print("ğŸ‰ TODOS LOS TESTS PASARON EXITOSAMENTE")
        print("\nâœ… El sistema estÃ¡ funcionando correctamente con modelos reales")
        print("âœ… Las velocidades medidas son con modelos descargados")
        print("âœ… Los benchmarks anteriores eran estimaciones, estos son REALES")
        return 0
    else:
        print(f"âš ï¸ {success_count}/{total_count} tests pasaron")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())
